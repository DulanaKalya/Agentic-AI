{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5669d8c0",
   "metadata": {},
   "source": [
    "# LangGraph + LLMs: OpenAI vs Nonâ€‘OpenAI Output Patterns\n",
    "\n",
    "This notebook summarizes everything we discussed about:\n",
    "\n",
    "1. How different LLM providers (OpenAI, Groq, Gemini) return outputs.\n",
    "2. Why your original LangGraph code worked only with OpenAI.\n",
    "3. How to write **providerâ€‘agnostic** code that works with *any* LLM.\n",
    "4. How to implement the **Reflexion Agent** style graph in two ways:\n",
    "   - Providerâ€‘agnostic (Groq / Gemini / OpenAI)\n",
    "   - OpenAIâ€‘tools style (classic examples)\n",
    "\n",
    "You can read this like notes + code templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de307d",
   "metadata": {},
   "source": [
    "## 1. The Core Contract: What LangGraph MessageGraph Expects\n",
    "\n",
    "LangGraph's `MessageGraph` has a simple contract:\n",
    "\n",
    "- **Node input:** `List[BaseMessage]`\n",
    "- **Node output:** `BaseMessage` *or* `List[BaseMessage]`\n",
    "\n",
    "Valid message types are the LangChain messages, e.g.:\n",
    "\n",
    "- `HumanMessage`\n",
    "- `AIMessage`\n",
    "- `ToolMessage`\n",
    "- (all subclasses of `BaseMessage`)\n",
    "\n",
    "ðŸ‘‰ If a node returns *anything else* (like a raw Pydantic model), LangGraph\n",
    "will throw an error like:\n",
    "\n",
    "> `Unsupported message type: <class 'schema.AnswerQuestion'>`\n",
    "\n",
    "This is what happened in your earlier code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791420e",
   "metadata": {},
   "source": [
    "## 2. Two Patterns of LLM Usage\n",
    "\n",
    "There are two main ways we used LLMs:\n",
    "\n",
    "### 2.1 OpenAI Tools Pattern\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: int, y: int):\n",
    "    return x + y\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools([add])\n",
    "\n",
    "resp = llm.invoke(\"add 5 and 7\")\n",
    "print(resp)\n",
    "```\n",
    "\n",
    "OpenAI returns an `AIMessage` with a `tool_calls` field, e.g.:\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "  content=\"\",\n",
    "  tool_calls=[\n",
    "    {\n",
    "      \"name\": \"add\",\n",
    "      \"args\": {\"x\": 5, \"y\": 7},\n",
    "      \"id\": \"call_123\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "```\n",
    "\n",
    "Your graph nodes can then read `last_ai_message.tool_calls`.\n",
    "\n",
    "### 2.2 Structured Output Pattern (Pydantic)\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\").with_structured_output(Person)\n",
    "\n",
    "resp = llm.invoke(\"Name Alice, age 25\")\n",
    "print(resp)\n",
    "```\n",
    "\n",
    "Here, `resp` is **not** an `AIMessage`. It is a Pydantic model:\n",
    "\n",
    "```python\n",
    "Person(name=\"Alice\", age=25)\n",
    "```\n",
    "\n",
    "If you plug this *directly* into `MessageGraph`, it will fail, because the\n",
    "graph expects `BaseMessage` objects, not arbitrary Python models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc441ac0",
   "metadata": {},
   "source": [
    "## 3. Why Your Old Code Worked Only with OpenAI\n",
    "\n",
    "Your original design assumed the **OpenAI tools pattern**:\n",
    "\n",
    "1. LLM returns `AIMessage` with `.tool_calls`.\n",
    "2. `execute_tools` reads `tool_calls` and produces `ToolMessage`s.\n",
    "3. Your loop (`event_loop`) counted `ToolMessage`s to decide when to stop.\n",
    "\n",
    "This depends on features that only OpenAI (and APIâ€‘compatible providers) expose.\n",
    "\n",
    "### Example of the old loop logic:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.graph import END\n",
    "\n",
    "MAX_ITERATIONS = 2\n",
    "\n",
    "def event_loop(state: list[BaseMessage]) -> str:\n",
    "    count_tool_visit = sum(isinstance(m, ToolMessage) for m in state)\n",
    "    if count_tool_visit > MAX_ITERATIONS:\n",
    "        return END\n",
    "    return \"execute_tools\"\n",
    "```\n",
    "\n",
    "This works only if `ToolMessage`s are actually being produced, which is true in\n",
    "the OpenAI tools pattern. But when you switched to Groq/Gemini + structured\n",
    "output, there were no `tool_calls` and often no `ToolMessage`s, so the count\n",
    "never increased. That caused an infinite loop and a `GraphRecursionError`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbc20f",
   "metadata": {},
   "source": [
    "## 4. Providerâ€‘Agnostic Pattern (Works with Groq / Gemini / OpenAI)\n",
    "\n",
    "To make your Reflexion agent work with **any** LLM, we did three things:\n",
    "\n",
    "1. Use `with_structured_output()` to get Pydantic models.\n",
    "2. Wrap those models into `AIMessage` manually.\n",
    "3. Implement loop logic (and tool usage) based on **your own fields**, not\n",
    "   on OpenAI's `tool_calls`.\n",
    "\n",
    "Below is a minimal providerâ€‘agnostic template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Pydantic schemas (these are the same for all providers)\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous.\")\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Answer the question.\"\"\"\n",
    "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    search_queries: List[str] = Field(\n",
    "        description=(\n",
    "            \"1-3 search queries for researching improvements to\"\n",
    "            \" address the critique of your current answer.\"\n",
    "        )\n",
    "    )\n",
    "    reflection: Reflection = Field(\n",
    "        description=\"Your reflection on the initial answer.\"\n",
    "    )\n",
    "\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    \"\"\"Revise your original answer to your question.\"\"\"\n",
    "    references: List[str] = Field(\n",
    "        description=\"Citations motivating your updated answer.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ed846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Draft and Revisor chains (LLM-agnostic setup)\n",
    "import datetime\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "\n",
    "# Example: here we would plug in Groq, Gemini, or OpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are expert AI researcher.\n",
    "Current time: {time}\n",
    "\n",
    "You must follow this process:\n",
    "\n",
    "1. {first_instruction}\n",
    "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "3. After the reflection, list 1â€“3 search queries separately for researching improvements.\n",
    "   - Do NOT include the search queries inside the reflection.\n",
    "4. Finally, answer the user's question above using the required format.\n",
    "\"\"\".strip(),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ").partial(time=lambda: datetime.datetime.now().isoformat())\n",
    "\n",
    "first_responder_prompt = actor_prompt_template.partial(\n",
    "    first_instruction=\"Provide a detailed ~250 word answer\"\n",
    ")\n",
    "\n",
    "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
    "    - You should use the previous critique to add important information to your answer.\n",
    "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "        - Add a \\\"References\\\" section to the bottom of your answer.\n",
    "    - You should use the previous critique to remove superfluous information from your answer\n",
    "      and make SURE it is not more than 250 words.\n",
    "\"\"\"\n",
    "\n",
    "revisor_prompt = actor_prompt_template.partial(\n",
    "    first_instruction=revise_instructions\n",
    ")\n",
    "\n",
    "# LLM with structured output\n",
    "first_responder_llm = llm.with_structured_output(AnswerQuestion)\n",
    "revisor_llm = llm.with_structured_output(ReviseAnswer)\n",
    "\n",
    "first_responder_chain = first_responder_prompt | first_responder_llm\n",
    "revisor_chain = revisor_prompt | revisor_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Draft node: Pydantic -> AIMessage\n",
    "def draft_node(state: list[BaseMessage]) -> list[BaseMessage]:\n",
    "    \"\"\"Use AnswerQuestion as *structured output*, then wrap into AIMessage.\n",
    "    This works with Groq, Gemini, OpenAI, etc.\n",
    "    \"\"\"\n",
    "    result: AnswerQuestion = first_responder_chain.invoke({\"messages\": state})\n",
    "\n",
    "    msg = AIMessage(\n",
    "        content=result.answer,\n",
    "        additional_kwargs={\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"reflection\": result.reflection.model_dump(),\n",
    "            \"role\": \"draft\",\n",
    "        },\n",
    "    )\n",
    "    return [msg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f49f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Tools node: use our own search_queries instead of tool_calls\n",
    "import json\n",
    "from typing import Dict, Any, List as PyList\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "tavily = TavilySearchResults(max_results=2)\n",
    "\n",
    "def execute_tools(state: PyList[BaseMessage]) -> PyList[BaseMessage]:\n",
    "    # Take the last AI message\n",
    "    last_ai = next((m for m in reversed(state) if isinstance(m, AIMessage)), None)\n",
    "    if not last_ai:\n",
    "        return []\n",
    "\n",
    "    search_queries = last_ai.additional_kwargs.get(\"search_queries\", [])\n",
    "    if not search_queries:\n",
    "        return []\n",
    "\n",
    "    query_results: Dict[str, Any] = {}\n",
    "    for q in search_queries:\n",
    "        # In a real run, this would call Tavily's API\n",
    "        query_results[q] = tavily.invoke(q)\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=json.dumps(query_results),\n",
    "        tool_call_id=\"manual-tools-1\",\n",
    "    )\n",
    "    return [tool_msg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Revisor node: structured ReviseAnswer -> AIMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def revisor_node(state: list[BaseMessage]) -> list[BaseMessage]:\n",
    "    # Count prior revisions\n",
    "    past_revisions = sum(\n",
    "        isinstance(m, AIMessage) and m.additional_kwargs.get(\"role\") == \"revisor\"\n",
    "        for m in state\n",
    "    )\n",
    "\n",
    "    result: ReviseAnswer = revisor_chain.invoke({\"messages\": state})\n",
    "\n",
    "    msg = AIMessage(\n",
    "        content=result.answer,\n",
    "        additional_kwargs={\n",
    "            \"role\": \"revisor\",\n",
    "            \"iteration\": past_revisions + 1,\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"reflection\": result.reflection.model_dump(),\n",
    "            \"references\": result.references,\n",
    "        },\n",
    "    )\n",
    "    return [msg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Loop logic: stop based on our own tags (provider-agnostic)\n",
    "from langgraph.graph import MessageGraph, END\n",
    "\n",
    "MAX_ITERATIONS = 2\n",
    "\n",
    "def event_loop(state: list[BaseMessage]) -> str:\n",
    "    # Count how many times revisor has run\n",
    "    num_iterations = sum(\n",
    "        isinstance(m, AIMessage) and m.additional_kwargs.get(\"role\") == \"revisor\"\n",
    "        for m in state\n",
    "    )\n",
    "\n",
    "    if num_iterations >= MAX_ITERATIONS:\n",
    "        return END\n",
    "\n",
    "    # Optional safety: if no search queries, just stop\n",
    "    last_ai = next((m for m in reversed(state) if isinstance(m, AIMessage)), None)\n",
    "    if not last_ai:\n",
    "        return END\n",
    "    if not last_ai.additional_kwargs.get(\"search_queries\"):\n",
    "        return END\n",
    "\n",
    "    return \"execute_tools\"\n",
    "\n",
    "# Wire the graph\n",
    "graph = MessageGraph()\n",
    "graph.add_node(\"draft\", draft_node)\n",
    "graph.add_node(\"execute_tools\", execute_tools)\n",
    "graph.add_node(\"revisor\", revisor_node)\n",
    "\n",
    "graph.add_edge(\"draft\", \"execute_tools\")\n",
    "graph.add_edge(\"execute_tools\", \"revisor\")\n",
    "graph.add_conditional_edges(\"revisor\", event_loop)\n",
    "graph.set_entry_point(\"draft\")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# Example usage (will error here if you don't have API keys/env set):\n",
    "print(\"Provider-agnostic Reflexion graph ready (not executed here).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fab58",
   "metadata": {},
   "source": [
    "## 5. OpenAI Tools Style Version\n",
    "\n",
    "Now, let's see how you would implement **the same idea** using OpenAI's\n",
    "native tools (`bind_tools`) instead of structured output.\n",
    "\n",
    "Key differences:\n",
    "\n",
    "- Use `ChatOpenAI(...).bind_tools([...])` instead of `with_structured_output`.\n",
    "- The LLM returns `AIMessage` with `.tool_calls`.\n",
    "- `execute_tools` reads `.tool_calls` instead of `search_queries` from\n",
    "  `additional_kwargs`.\n",
    "- Your stop condition can be based on `ToolMessage` count *or* your own tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 OpenAI-specific setup (pseudo-code; requires OPENAI_API_KEY)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, ToolMessage, AIMessage\n",
    "\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "actor_prompt_template_oa = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are expert AI researcher.\n",
    "Current time: {time}\n",
    "\n",
    "You must follow this process:\n",
    "1. {first_instruction}\n",
    "2. Reflect and critique your answer.\n",
    "3. After the reflection, list 1â€“3 search queries separately for researching improvements.\n",
    "\"\"\".strip(),\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "]).partial(time=lambda: datetime.datetime.now().isoformat())\n",
    "\n",
    "first_responder_prompt_oa = actor_prompt_template_oa.partial(\n",
    "    first_instruction=\"Provide a detailed ~250 word answer\"\n",
    ")\n",
    "\n",
    "revisor_prompt_oa = actor_prompt_template_oa.partial(\n",
    "    first_instruction=\"Revise your previous answer using the new information.\"\n",
    ")\n",
    "\n",
    "# Now bind tools instead of structured output\n",
    "first_responder_chain_oa = first_responder_prompt_oa | openai_llm.bind_tools(\n",
    "    tools=[AnswerQuestion],\n",
    "    tool_choice=\"AnswerQuestion\",  # force this tool\n",
    ")\n",
    "\n",
    "revisor_chain_oa = revisor_prompt_oa | openai_llm.bind_tools(\n",
    "    tools=[ReviseAnswer],\n",
    "    tool_choice=\"ReviseAnswer\",\n",
    ")\n",
    "\n",
    "print(\"OpenAI tool-calling chains defined (not executed here).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 execute_tools for OpenAI: read .tool_calls\n",
    "from typing import List as PyList, Dict, Any\n",
    "import json\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "def execute_tools_openai(state: PyList[BaseMessage]) -> PyList[BaseMessage]:\n",
    "    last_ai = state[-1]\n",
    "    if not isinstance(last_ai, AIMessage):\n",
    "        return []\n",
    "\n",
    "    if not getattr(last_ai, \"tool_calls\", None):\n",
    "        return []\n",
    "\n",
    "    tool_messages: PyList[ToolMessage] = []\n",
    "\n",
    "    for tool_call in last_ai.tool_calls:\n",
    "        name = tool_call[\"name\"]\n",
    "        args = tool_call[\"args\"]\n",
    "        call_id = tool_call[\"id\"]\n",
    "\n",
    "        if name in [\"AnswerQuestion\", \"ReviseAnswer\"]:\n",
    "            search_queries = args.get(\"search_queries\", [])\n",
    "            query_results: Dict[str, Any] = {}\n",
    "            for q in search_queries:\n",
    "                query_results[q] = tavily_tool.invoke(q)\n",
    "\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(query_results),\n",
    "                    tool_call_id=call_id,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return tool_messages\n",
    "\n",
    "print(\"OpenAI execute_tools defined (not executed here).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd761108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Example of loop condition using ToolMessage count (OpenAI style)\n",
    "from langgraph.graph import MessageGraph, END\n",
    "\n",
    "MAX_ITERATIONS_OA = 2\n",
    "\n",
    "def event_loop_openai(state: list[BaseMessage]) -> str:\n",
    "    count_tool_visit = sum(isinstance(m, ToolMessage) for m in state)\n",
    "    if count_tool_visit >= MAX_ITERATIONS_OA:\n",
    "        return END\n",
    "    return \"execute_tools_openai\"\n",
    "\n",
    "graph_oa = MessageGraph()\n",
    "graph_oa.add_node(\"draft\", first_responder_chain_oa)\n",
    "graph_oa.add_node(\"execute_tools_openai\", execute_tools_openai)\n",
    "graph_oa.add_node(\"revisor\", revisor_chain_oa)\n",
    "\n",
    "graph_oa.add_edge(\"draft\", \"execute_tools_openai\")\n",
    "graph_oa.add_edge(\"execute_tools_openai\", \"revisor\")\n",
    "graph_oa.add_conditional_edges(\"revisor\", event_loop_openai)\n",
    "graph_oa.set_entry_point(\"draft\")\n",
    "\n",
    "app_oa = graph_oa.compile()\n",
    "print(\"OpenAI-based Reflexion graph ready (not executed here).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d8137",
   "metadata": {},
   "source": [
    "## 6. Summary: When to Use Which Pattern\n",
    "\n",
    "- **If you use OpenAI and want tight tool integration**:\n",
    "  - Use `bind_tools`.\n",
    "  - Consume `AIMessage.tool_calls`.\n",
    "  - Optionally base loop logic on `ToolMessage` count.\n",
    "\n",
    "- **If you use Groq, Gemini, or want providerâ€‘agnostic code**:\n",
    "  - Use `with_structured_output(PydanticModel)`.\n",
    "  - Wrap the result into `AIMessage` manually.\n",
    "  - Store metadata (search_queries, role, iteration, reflection, references)\n",
    "    in `additional_kwargs`.\n",
    "  - Implement `execute_tools` and loop conditions based on those fields.\n",
    "\n",
    "If you follow these patterns, you will avoid the common errors:\n",
    "\n",
    "- `Unsupported message type: <class '...'>`\n",
    "- `GraphRecursionError: Recursion limit reached without hitting a stop condition`\n",
    "\n",
    "and your agent graphs will work across different LLM providers."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
